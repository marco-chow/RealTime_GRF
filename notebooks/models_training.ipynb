{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MPS as the device\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "data_folder = os.path.join(parent_path, \"data\")\n",
    "raw_data_path = os.path.join(data_folder, \"parquet\", \"raw\", \"raw.parquet\")\n",
    "df = pd.read_parquet(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop T01 - T03 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropids = [\n",
    "    id for id in df.trial_id.unique() if any(x in id for x in [\"T01\", \"T02\", \"T03\"])\n",
    "]\n",
    "df = df.loc[~df[\"trial_id\"].isin(dropids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "non_id_cols = [\n",
    "    \"index\",\n",
    "    \"acc_1\",\n",
    "    \"acc_2\",\n",
    "    \"acc_3\",\n",
    "    \"gyr_1\",\n",
    "    \"gyr_2\",\n",
    "    \"gyr_3\",\n",
    "    \"grf_1\",\n",
    "    \"grf_2\",\n",
    "    \"grf_3\",\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[non_id_cols] = scaler.fit_transform(df[non_id_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"acc_1\", \"acc_2\", \"acc_3\", \"gyr_1\", \"gyr_2\", \"gyr_3\"]\n",
    "target_cols = [\"grf_1\", \"grf_2\", \"grf_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:148\n",
      "Val Size:19\n",
      "Test Size:19\n",
      " \n",
      "Input Shape: (4500, 6), Output Shape: (4500, 3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trial_id_list = df_scaled.trial_id.unique().tolist()\n",
    "\n",
    "train_ids, test_ids = train_test_split(trial_id_list, test_size=0.2)\n",
    "val_ids, test_ids = train_test_split(test_ids, test_size=0.5)\n",
    "\n",
    "print(\n",
    "    f\"Train Size:{len(train_ids)}\\nVal Size:{len(val_ids)}\\nTest Size:{len(test_ids)}\\n \"\n",
    ")\n",
    "\n",
    "# Separate data into the three dfs\n",
    "\n",
    "train_df = df_scaled.loc[df_scaled.trial_id.isin(train_ids)]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "val_df = df_scaled.loc[df_scaled.trial_id.isin(val_ids)]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "test_df = df_scaled.loc[df_scaled.trial_id.isin(test_ids)]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Define Function to split the dfs into X and Y sequences\n",
    "def prep_sequences(df):\n",
    "\n",
    "    df_grouped = df.groupby(\"trial_id\")\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    for _, trial in df_grouped:\n",
    "        X_list.append(trial[feature_cols].values)\n",
    "        Y_list.append(trial[target_cols].values)\n",
    "\n",
    "    return X_list, Y_list\n",
    "\n",
    "\n",
    "X_train, Y_train = prep_sequences(train_df)\n",
    "X_val, Y_val = prep_sequences(val_df)\n",
    "X_test, Y_test = prep_sequences(test_df)\n",
    "\n",
    "\n",
    "# Create the Torch Datasets\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "train_dataset = CreateDataset(X_train, Y_train)\n",
    "val_dataset = CreateDataset(X_val, Y_val)\n",
    "test_dataset = CreateDataset(X_test, Y_test)\n",
    "\n",
    "# Check datashape\n",
    "sample_input, sample_output = train_dataset[0]  # Get the first sample\n",
    "print(f\"Input Shape: {sample_input.shape}, Output Shape: {sample_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Define collate function for Dataloader\n",
    "def collate_fn(batch):\n",
    "\n",
    "    inputs, outputs = zip(*batch)\n",
    "\n",
    "    # Convert to tensors\n",
    "    inputs = [torch.tensor(seq, dtype=torch.float32) for seq in inputs]\n",
    "    outputs = [torch.tensor(seq, dtype=torch.float32) for seq in outputs]\n",
    "\n",
    "    # Sort the sequences in descending orders\n",
    "    lengths = torch.tensor([len(seq) for seq in inputs])  # Get length of each sequence\n",
    "    lengths, sorted_idx = lengths.sort(descending=True)\n",
    "\n",
    "    inputs = [inputs[i] for i in sorted_idx]\n",
    "    outputs = [outputs[i] for i in sorted_idx]\n",
    "\n",
    "    # Pad the sorted sequence\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=-100)\n",
    "    outputs_padded = pad_sequence(outputs, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return inputs_padded, outputs_padded, lengths\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Sort the lengths in descending order and get the sorted indices\n",
    "        lengths, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x = x[sorted_idx]  # Sort the input accordingly\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        out_packed, _ = self.rnn(x_packed)\n",
    "\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.8592, Validation Loss: 0.6256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     80\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[132], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, val_dataloader, num_epochs, early_stop_patience)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/DS/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define model training function\n",
    "def train_model(\n",
    "    model, dataloader, val_dataloader, num_epochs=50, early_stop_patience=10\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        best_val_loss = np.inf\n",
    "        patience_count = 0\n",
    "\n",
    "        for inputs, targets, lengths in dataloader:\n",
    "            inputs, targets, lengths = (\n",
    "                inputs.to(device),\n",
    "                targets.to(device),\n",
    "                lengths.cpu(),\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(inputs, lengths)\n",
    "\n",
    "            # Mask padded values\n",
    "            masks = (targets != -100).float()\n",
    "            predictions = predictions * masks\n",
    "            targets = targets * masks\n",
    "\n",
    "            # Loss Calculation\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss = loss * masks  # Mask out padded positions\n",
    "            loss = loss.sum() / masks.sum()  # Normalize by valid tokens\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(dataloader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "\n",
    "        val_loss = eval_model(model, val_dataloader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_count = 0\n",
    "        else:\n",
    "            patience_count += 1\n",
    "\n",
    "        if patience_count == early_stop_patience:\n",
    "            print(f\"Early Stop Triggered: Best Validation Loss = {best_val_loss}\")\n",
    "            return\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    return model, training_losses, best_val_loss\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for inputs, targets, lengths in dataloader:\n",
    "        inputs, targets, lengths = inputs.to(device), targets.to(device), lengths.cpu()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(inputs, lengths)\n",
    "\n",
    "        # Mask padded values\n",
    "        masks = (targets != -100).float()\n",
    "        predictions = predictions * masks\n",
    "        targets = targets * masks\n",
    "\n",
    "        # Loss Calculation\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss = loss * masks\n",
    "        loss = loss.sum() / masks.sum()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = epoch_loss / len(dataloader)\n",
    "\n",
    "    return avg_eval_loss\n",
    "\n",
    "\n",
    "input_size = X_train[0].shape[1]\n",
    "output_size = Y_train[0].shape[1]\n",
    "model = RNNModel(\n",
    "    input_size=input_size, hidden_size=64, output_size=output_size\n",
    ")  # Adjust sizes as needed\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(\n",
    "    model, train_dataloader, val_dataloader, num_epochs=50, early_stop_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
